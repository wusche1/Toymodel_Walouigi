{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2-medium\"  # you can replace 'gpt2-medium' with 'gpt2' if you want the smaller model\n",
    "\n",
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_avg_prob_embedding(model, logits, attention_mask=None):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    device = model.device  # This line captures the device model is on\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits.to(device), dim=-1)  # Ensure logits are on the same device\n",
    "\n",
    "    # Get all possible embeddings from the model\n",
    "    all_embeddings = model.transformer.wte.weight\n",
    "\n",
    "    weighted_embeddings = torch.matmul(probabilities, all_embeddings)\n",
    "    weighted_embeddings = weighted_embeddings.unsqueeze(0)\n",
    "\n",
    "    hidden_states = weighted_embeddings\n",
    "\n",
    "    # Pass the repeated averaged embeddings through the model layers\n",
    "    for block in model.transformer.h:  # Updated loop to iterate through GPT2 blocks\n",
    "        hidden_states = block(hidden_states)[0]  # Taking the first output as the transformed hidden state\n",
    "\n",
    "    # Convert the final hidden states back to logits\n",
    "    logits_output = model.transformer.ln_f(hidden_states)  # Apply final layer norm\n",
    "    logits_output = model.transformer.wte.weight.matmul(logits_output.transpose(1, 2)).transpose(1, 2)  # Convert back to logits\n",
    "\n",
    "    # Return the logits\n",
    "    return logits_output.squeeze(0)  # Removing the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logit_tensor_from_prompt(prompt: str, tokenizer):\n",
    "    \"\"\"\n",
    "    Given a prompt, tokenizes the prompt using the provided tokenizer and returns a logit tensor \n",
    "    where positions corresponding to the tokens have a value of 0 and all other positions have a value of -inf.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    tokenized_output = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()\n",
    "    \n",
    "    # Check if tokenized_output is a scalar\n",
    "    if len(tokenized_output.shape) == 0:\n",
    "        tokenized_output = tokenized_output.unsqueeze(0)  # Convert scalar to 1D tensor\n",
    "    \n",
    "    tokenized_output = tokenized_output.tolist()\n",
    "    \n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    tensor = t.full((len(tokenized_output), vocab_size), float('-inf'))\n",
    "    \n",
    "    for idx, token in enumerate(tokenized_output):\n",
    "        tensor[idx, token] = 0.0\n",
    "        \n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kl_divergence_from_logits(logits1, logits2):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two distributions represented by logits.\n",
    "    \n",
    "    Args:\n",
    "    - logits1 (list or torch.Tensor): Logits for the first distribution.\n",
    "    - logits2 (list or torch.Tensor): Logits for the second distribution.\n",
    "    \n",
    "    Returns:\n",
    "    - KL divergence (torch.Tensor): KL(logits1 || logits2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    logits1 = torch.tensor(logits1)\n",
    "    logits2 = torch.tensor(logits2)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs1 = F.softmax(logits1, dim=-1)\n",
    "    probs2 = F.softmax(logits2, dim=-1)\n",
    "\n",
    "    # Compute KL divergence\n",
    "    kl_div = F.kl_div(probs1, probs2, reduction='batchmean')\n",
    "\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_strings(N, M, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate N strings each with M tokens using GPT-2.\n",
    "\n",
    "    Args:\n",
    "    - N (int): Number of strings to generate.\n",
    "    - M (int): Number of tokens per string.\n",
    "    - model (GPT2LMHeadModel): Pre-trained GPT-2 model.\n",
    "    - tokenizer (GPT2Tokenizer): Tokenizer for GPT-2.\n",
    "\n",
    "    Returns:\n",
    "    - List of N strings.\n",
    "    \"\"\"\n",
    "\n",
    "    generated_strings = []\n",
    "    \n",
    "    # Set model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # If bos_token is not available in the tokenizer, use a random token (e.g., \"a\")\n",
    "    input_token = tokenizer.bos_token or \"a\"\n",
    "    input_ids = tokenizer.encode(input_token, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    while len(generated_strings)!=N:\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            output = model.generate(input_ids, max_length=M+1, do_sample=True, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "        \n",
    "        # Convert the generated tokens to a string\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        if len(tokenizer.encode(generated_text)) == M:\n",
    "            generated_strings.append(generated_text)\n",
    "    \n",
    "\n",
    "    return generated_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(model, tokenizer,Number_of_completions,Length_of_completions,N_samples):\n",
    "    kl_divs_linearity=t.zeros((Number_of_completions,Length_of_completions,N_samples))\n",
    "    avg_kl_divs=t.zeros((Number_of_completions,Length_of_completions,N_samples))\n",
    "    std_kl_divs=t.zeros((Number_of_completions,Length_of_completions,N_samples))\n",
    "    for N in range(Number_of_completions):\n",
    "        for M in range(Length_of_completions):\n",
    "            for O in range(N_samples):\n",
    "                strings = generate_strings(N+1, M+1, model, tokenizer)\n",
    "                logits=[create_logit_tensor_from_prompt(s,tokenizer) for s in strings]\n",
    "                average_input_logits = torch.log(torch.mean(torch.stack([torch.nn.functional.softmax(logit, dim=-1) for logit in logits]), dim=0))\n",
    "\n",
    "                output_logits=[process_with_avg_prob_embedding(model, l) for l in logits]\n",
    "                output_logits_averagred_before_forwardpass=process_with_avg_prob_embedding(model, average_input_logits)\n",
    "                output_logits_averagred_after_forwardpass=torch.stack(output_logits).mean(dim=0)\n",
    "\n",
    "                kl_between_members=[]\n",
    "                for i in range(len(output_logits)):\n",
    "                    for j in range(len(output_logits)):\n",
    "                        if i!=j:\n",
    "                            div=kl_divergence_from_logits(output_logits[i][-1], output_logits[j][-1])\n",
    "                            kl_between_members.append(float(div.item()))\n",
    "                kl_on_linearity=kl_divergence_from_logits(output_logits_averagred_before_forwardpass[-1], output_logits_averagred_after_forwardpass[-1]).item()\n",
    "                \n",
    "                kl_divs_linearity[N,M,O]=kl_on_linearity\n",
    "                avg_kl_divs[N,M,O]=np.mean(kl_between_members)\n",
    "                std_kl_divs[N,M,O]=np.std(kl_between_members)\n",
    "    return kl_divs_linearity,avg_kl_divs,std_kl_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divs_linearity,avg_kl_divs,std_kl_divs=run_experiments(model, tokenizer,5,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinarity=t.mean(kl_divs_linearity,dim=2)\n",
    "avg_normal=t.mean(avg_kl_divs,dim=2)\n",
    "result_tensor = nonlinarity / avg_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(result_tensor, origin='lower')\n",
    "plt.colorbar()  # Add a colorbar\n",
    "\n",
    "# Add axis descriptions\n",
    "plt.ylabel('Number of Completions')\n",
    "plt.xlabel('Completion length')\n",
    "plt.title('KL Divergence(linear combination before vs. after worwardpass)\\n /average KL divergence amungst different prompts')\n",
    "\n",
    "# Adjust the ticks\n",
    "plt.xticks(range(result_tensor.shape[1]), [str(i+1) for i in range(result_tensor.shape[1])])\n",
    "plt.yticks(range(result_tensor.shape[0]), [str(i+1) for i in range(result_tensor.shape[0])])\n",
    "#y_ticks = plt.yticks()[0]  # Get current ticks\n",
    "#plt.yticks(y_ticks, y_ticks + 1)  # Adjust ticks to start from 1\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
